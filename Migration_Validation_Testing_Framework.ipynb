{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36279013-e1ca-4f7a-9f7e-b4a2b941b62a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "notebook_start_datetime=datetime.now()\n",
    "notebook_start_date=str(notebook_start_datetime)\n",
    "notebook_start_date=notebook_start_date[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a607440b-436f-4993-b42c-1c8feab418e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# CREATE SCHEMA IF NOT EXISTS generic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6fb19b-467b-45ab-966f-28dd332c7099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# CREATE OR REPLACE TABLE generic.tbl_generic_testing_results (\n",
    "#     Test_ID BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "#     Databricks_View_Name STRING,\n",
    "#     SAP_View_Name STRING,\n",
    "#     Test_Name STRING,\n",
    "#     Test_Description STRING,\n",
    "#     Test_Start_Date STRING,\n",
    "#     Test_End_Date STRING,\n",
    "#     Test_Execution_Time_In_Seconds STRING,\n",
    "#     Databricks_View_Count STRING,\n",
    "#     SAP_View_Count STRING,\n",
    "#     Check_Difference_Count_Between_Databricks_And_SAP STRING,\n",
    "#     Check_Difference_Sum_Between_Databricks_And_SAP STRING,\n",
    "#     Test_validation_Status STRING,\n",
    "#     Comment STRING  \n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34e07a91-2941-4808-9ace-3c490698ef60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks_view_name = dbutils.widgets.get(\"databricks_view_name\")\n",
    "SAP_View_Name = dbutils.widgets.get(\"SAP_View_Name\")\n",
    "databricks_database_name = dbutils.widgets.get(\"databricks_database_name\")\n",
    "sap_column_count = int(dbutils.widgets.get(\"sap_column_count\"))\n",
    "sap_row_count = int(dbutils.widgets.get(\"sap_row_count\"))\n",
    "\n",
    "sap_dup_record_count = int(dbutils.widgets.get(\"sap_duplicate_record_count\"))\n",
    "sap_col_name = dbutils.widgets.get(\"sap_null_column_name_list\").split(',')\n",
    "#########################################\n",
    "sap_col_null_value_count =dbutils.widgets.get(\"sap_null_column_value_list\").split(',')\n",
    "list1=[]\n",
    "for x in sap_col_null_value_count:\n",
    "  list1.append(int(x))\n",
    "sap_col_null_value_count=list1\n",
    "############################################\n",
    "sap_column_name_lst= dbutils.widgets.get(\"sap_sum_column_name_list\").split(',')\n",
    "############################################\n",
    "sap_column_sum_lst = dbutils.widgets.get(\"sap_sum_column_value_list\").split(',')\n",
    "list1=[]\n",
    "for x in sap_column_sum_lst:\n",
    "  list1.append(float(x))\n",
    "sap_column_sum_lst=list1\n",
    "###############################################\n",
    "execution_time_of_sap_view=dbutils.widgets.get(\"execution_time_of_sap_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774683b1-3e27-4d3d-b0a0-3aed7b4dc951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#generating dataframe for original view\n",
    "\n",
    "def convert_DF (databricks_view_name,databricks_database_name):\n",
    "  df = spark.sql(f\"select * from {databricks_database_name}.{databricks_view_name}\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cfc6a4-df52-4f45-b083-ab3a9ac63e43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calling function of converting dataframe\n",
    "try:\n",
    "    databricks_view_df=convert_DF (databricks_view_name,databricks_database_name)\n",
    "except:\n",
    "    start_time=datetime.now()\n",
    "    start_date=str(start_time)\n",
    "    start_date=start_date[0:19]\n",
    "    spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                hive_metastore.generic.tbl_generic_testing_results \n",
    "                (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                VALUES\n",
    "                ('{databricks_view_name}','','','Error Found while converting to DataFrame','{start_date}','','','','','','','FAIL','ExceptionRaised: Table or view not found : please check view name and database name')\n",
    "        \"\"\"\n",
    "    )\n",
    "    dbutils.notebook.exit(\"ExceptionRaised: Table or view not found : please check view name and database name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae564351-951f-442b-b4c5-1ffb5afe525e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Row count validation Testing\n",
    "def row_count_validation(databricks_view_df,sap_row_count):\n",
    "    start_time=datetime.now()\n",
    "    start_date=str(start_time)\n",
    "    start_date=start_date[0:19]\n",
    "    row_count_databricks_view = databricks_view_df.count()\n",
    "    row_count_matched = str(row_count_databricks_view == sap_row_count)\n",
    "    diff_row_count=str(abs(row_count_databricks_view - sap_row_count))\n",
    "    row_count_databricks_view=str(row_count_databricks_view)\n",
    "    sap_row_count=str(sap_row_count)\n",
    "    end_time=datetime.now()\n",
    "    end_date=str(end_time)\n",
    "    end_date=end_date[0:19]\n",
    "    execution_time=str(end_time-start_time)\n",
    "    execution_time=execution_time[5:]\n",
    "    \n",
    "    if row_count_matched =='True':\n",
    "      spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Row Count','Row count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{row_count_databricks_view}','{sap_row_count}','{diff_row_count}','','PASS','')\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "      spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Row Count','Row count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{row_count_databricks_view}','{sap_row_count}','{diff_row_count}','','FAIL','')\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return row_count_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6504e3f1-37d1-4a8e-942e-a0b204745767",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count_validation=row_count_validation(databricks_view_df,sap_row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e337b5-900f-4302-97d3-5b2801999865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Column Count validation testing\n",
    "\n",
    "def number_of_columns (databricks_view_df,sap_column_count):\n",
    "  start_time=datetime.now()\n",
    "  start_date=str(start_time)\n",
    "  start_date=start_date[0:19]\n",
    "  databricks_col_count=len(databricks_view_df.columns)\n",
    "  difference_col_count=str(abs(databricks_col_count-sap_column_count))\n",
    "  col_count_val_status=str(databricks_col_count==sap_column_count)\n",
    "  sap_column_count=str(sap_column_count)\n",
    "  end_time=datetime.now()\n",
    "  end_date=str(end_time)\n",
    "  end_date=end_date[0:19]\n",
    "  execution_time=str(end_time-start_time)\n",
    "  execution_time=execution_time[5:]\n",
    "  \n",
    "  if col_count_val_status=='True':\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Column Count','Column count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{databricks_col_count}','{sap_column_count}','{difference_col_count}','','PASS','')\n",
    "          \"\"\"\n",
    "      )\n",
    "  else:\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Column Count','Column count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{databricks_col_count}','{sap_column_count}','{difference_col_count}','','FAIL','')\n",
    "          \"\"\"\n",
    "      )\n",
    "\n",
    "  return col_count_val_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3281873-05fb-447a-8231-c94127d6d5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_count_val_status=number_of_columns(databricks_view_df,sap_column_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c855806-9599-4de9-a44e-b45c473d942c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Duplicate records count validation testing\n",
    "import pyspark.sql.functions as f\n",
    "def Count_duplicate_records(databricks_view_df,sap_dup_record_count):\n",
    "  start_time=datetime.now()\n",
    "  start_date=str(start_time)\n",
    "  start_date=start_date[0:19]\n",
    "  \n",
    "  databricks_dup_record_count=(databricks_view_df.groupBy(databricks_view_df.columns).agg((f.count(\"*\")>1).cast(\"int\").alias(\"dup_cnt\")).where(f.col(\"dup_cnt\") > 1)).count()\n",
    "\n",
    "\n",
    "  diff_dup_count=str(abs(databricks_dup_record_count-sap_dup_record_count))\n",
    "  duplicate_record_val_status=str(databricks_dup_record_count==sap_dup_record_count)\n",
    "  databricks_dup_record_count=str(databricks_dup_record_count)\n",
    "  sap_dup_record_count=str(sap_dup_record_count)\n",
    "  end_time=datetime.now()\n",
    "  end_date=str(end_time)\n",
    "  end_date=end_date[0:19]\n",
    "  execution_time=str(end_time-start_time)\n",
    "  execution_time=execution_time[5:]\n",
    "\n",
    "  if duplicate_record_val_status=='True':\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Duplicate Record Count','Duplicate record count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{databricks_dup_record_count}','{sap_dup_record_count}','{diff_dup_count}','','PASS','')\n",
    "          \"\"\"\n",
    "      )\n",
    "  else:\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Duplicate Record Count','Duplicate record count validation of databricks and SAP view','{start_date}','{end_date}','{execution_time}','{databricks_dup_record_count}','{sap_dup_record_count}','{diff_dup_count}','','FAIL','')\n",
    "          \"\"\"\n",
    "      )\n",
    "  return duplicate_record_val_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f35100-d12c-4678-b4c8-c611885bf824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_record_val_status=Count_duplicate_records(databricks_view_df,sap_dup_record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee55c5d-b977-4e0a-a661-d9492af6d791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Null value count Validation\n",
    "from pyspark.sql.functions import isnan, when, count, sum\n",
    "def null_value_counter(databricks_view_df,sap_col_name,sap_col_null_value_count):\n",
    "  start_time=datetime.now()\n",
    "  start_date=str(start_time)\n",
    "  start_date=start_date[0:19]\n",
    "\n",
    "  from pyspark.sql.types import StringType \n",
    "  spark_df = databricks_view_df.select([col(c).cast(StringType()).alias(c) for c in sap_col_name])\n",
    "  spark_df=spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sap_col_name])\n",
    "  val=spark_df.collect()[0]\n",
    "  databricks_col_null_val=[c for c in val]\n",
    "\n",
    "  sap_col_name_str=\"\"\n",
    "  if len(sap_col_name)>=1:\n",
    "    sap_col_name_str=sap_col_name[0]\n",
    "    if len(sap_col_name)>1:\n",
    "      for i in range(1,len(sap_col_name)):\n",
    "        sap_col_name_str=sap_col_name_str+', '+sap_col_name[i]      \n",
    "  \n",
    "  col_null_value_count_fail_name=\"\"\n",
    "  for i in range(len(sap_col_null_value_count)):\n",
    "    if sap_col_null_value_count[i]==databricks_col_null_val[i]:\n",
    "      col_null_value_count_validate_status='True'\n",
    "    else:\n",
    "      col_null_value_count_validate_status='False'\n",
    "      databricks_count=str(databricks_col_null_val[i])\n",
    "      sap_count=str(sap_col_null_value_count[i])\n",
    "      diff_count=str(abs(databricks_col_null_val[i]-sap_col_null_value_count[i]))\n",
    "      col_null_value_count_fail_name=sap_col_name[i]\n",
    "      break\n",
    "\n",
    "  end_time=datetime.now()\n",
    "  end_date=str(end_time)\n",
    "  end_date=end_date[0:19]\n",
    "  execution_time=str(end_time-start_time)\n",
    "  execution_time=execution_time[5:]\n",
    "\n",
    "  if col_null_value_count_validate_status=='True':\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Null Value Count',concat('Null value count on ','{sap_col_name_str}',' columns'),'{start_date}','{end_date}','{execution_time}','','','0','','PASS','')\n",
    "          \"\"\"\n",
    "      )\n",
    "  else:\n",
    "    spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Null Value Count',concat('Null value count on ','{sap_col_name_str}',' columns'),'{start_date}','{end_date}','{execution_time}','{databricks_count}','{sap_count}','{diff_count}','','FAIL',concat('{col_null_value_count_fail_name}',' column null count is not matching'))\n",
    "          \"\"\"\n",
    "      )\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf3dc26-c488-4dbd-bf40-4f7d64ee6105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if sap_col_name[0]!=\"NO_COLUMN\" :\n",
    "  if len(sap_col_name)!=0 and len(sap_col_null_value_count)!=0:\n",
    "    from pyspark.sql.functions import col\n",
    "    sap_col_name_str=\"\"\n",
    "    if len(sap_col_name)>=1:\n",
    "      sap_col_name_str=sap_col_name[0]\n",
    "      if len(sap_col_name)>1:\n",
    "        for i in range(1,len(sap_col_name)):\n",
    "          sap_col_name_str=sap_col_name_str+', '+sap_col_name[i]\n",
    "    if len(sap_col_name)!=0 and len(sap_col_null_value_count)!=0 and len(sap_col_name)==len (sap_col_null_value_count):\n",
    "      try:\n",
    "        null_value_counter(databricks_view_df,sap_col_name,sap_col_null_value_count)\n",
    "      except:\n",
    "        start_time=datetime.now()\n",
    "        start_date=str(start_time)\n",
    "        start_date=start_date[0:19]\n",
    "\n",
    "        spark.sql(\n",
    "              f\"\"\"\n",
    "                  INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Null Value Count',concat('Null value count on ','{sap_col_name_str}',' columns'),'{start_date}','','','','','','','FAIL','Error occured while validating null record count on columns')\n",
    "            \"\"\"\n",
    "        )\n",
    "  else:\n",
    "      start_time=datetime.now()\n",
    "      start_date=str(start_time)\n",
    "      start_date=start_date[0:19]\n",
    "    \n",
    "      spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Null Value Count',concat('Null value count on ','{sap_col_name_str}',' columns'),'{start_date}','','','','','','','FAIL','Error occured while validating null record count please check list of parameters')\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acf1a0bf-6b55-4718-a44c-a6899d932158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sum of measured columns\n",
    "sap_col_name_str=sap_column_name_lst[0]\n",
    "if len(sap_col_name_str)>1:\n",
    "  for i in range(1,len(sap_column_name_lst)):\n",
    "    sap_col_name_str=sap_col_name_str+', '+sap_column_name_lst[i]\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import sum\n",
    "def sum_of_column(databricks_view_df, sap_column_name_lst, sap_column_sum_lst):\n",
    "  \n",
    "  sum_df=databricks_view_df.na.fill(0).select([sum(c) for c in sap_column_name_lst])\n",
    "  val=sum_df.collect()[0]\n",
    "  databricks_col_sum_lst=[float(c) for c in val]\n",
    "\n",
    "  for i in range(0,len(sap_column_name_lst)):\n",
    "    start_time=datetime.now()\n",
    "    start_date=str(start_time)\n",
    "    start_date=start_date[0:19]\n",
    "    sap_col_name_str=str(sap_column_name_lst[i])\n",
    "    if abs(sap_column_sum_lst[i])==abs(databricks_col_sum_lst[i]):\n",
    "      end_time=datetime.now()\n",
    "      end_date=str(end_time)\n",
    "      end_date=end_date[0:19]\n",
    "      execution_time=str(end_time-start_time)\n",
    "      execution_time=execution_time[5:]\n",
    "      spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Sum of measured columns',concat('Validating Sum on ','{sap_col_name_str}', ' column'),'{start_date}','{end_date}','{execution_time}','','','','0','PASS','')\n",
    "          \"\"\"\n",
    "      )\n",
    "    else:\n",
    "      start_time=datetime.now()\n",
    "      start_date=str(start_time)\n",
    "      start_date=start_date[0:19]\n",
    "      sap_col_name_str=str(sap_column_name_lst[i])\n",
    "      databricks_sum=str(databricks_col_sum_lst[i])\n",
    "      sap_count=str(sap_column_sum_lst[i])\n",
    "      diff_count=str(abs(abs(databricks_col_sum_lst[i])-abs(sap_column_sum_lst[i])))\n",
    "      sum_fail_col_name=str(sap_column_name_lst[i])\n",
    "      end_time=datetime.now()\n",
    "      end_date=str(end_time)\n",
    "      end_date=end_date[0:19]\n",
    "      execution_time=str(end_time-start_time)\n",
    "      execution_time=execution_time[5:]\n",
    "      spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Sum of measured columns',concat('Validating Sum on ','{sap_col_name_str}', ' column'),'{start_date}','{end_date}','{execution_time}','','','','{diff_count}','FAIL',concat('{sum_fail_col_name}',' column sum is not matching'))\n",
    "          \"\"\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4cededd-92cd-4839-b4f8-11e25b0b94af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1901177538400501>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msap_column_name_lst\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m!=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m      2\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_name_lst)\u001B[38;5;241m!=\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_sum_lst)\u001B[38;5;241m!=\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_name_lst)\u001B[38;5;241m==\u001B[39m\u001B[38;5;28mlen\u001B[39m(sap_column_sum_lst):\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'int' object is not subscriptable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1901177538400501>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msap_column_name_lst\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m!=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      2\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_name_lst)\u001B[38;5;241m!=\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_sum_lst)\u001B[38;5;241m!=\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sap_column_name_lst)\u001B[38;5;241m==\u001B[39m\u001B[38;5;28mlen\u001B[39m(sap_column_sum_lst):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\n\u001B[0;31mTypeError\u001B[0m: 'int' object is not subscriptable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'int' object is not subscriptable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if sap_column_name_lst[0]!=\"NO_COLUMN\":\n",
    "  if len(sap_column_name_lst)!=0 and len(sap_column_sum_lst)!=0 and len(sap_column_name_lst)==len(sap_column_sum_lst):\n",
    "    try:\n",
    "      sum_of_column(databricks_view_df, sap_column_name_lst, sap_column_sum_lst)\n",
    "    except:\n",
    "      start_time=datetime.now()\n",
    "      start_date=str(start_time)\n",
    "      start_date=start_date[0:19]\n",
    "      spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Sum of measured columns',concat('Validating Sum on ','{sap_col_name_str}', ' columns'),'{start_date}','','','','','','','FAIL','Error occur while validating sum of measured columns')\n",
    "            \"\"\"\n",
    "        )\n",
    "  else:\n",
    "      start_time=datetime.now()\n",
    "      start_date=str(start_time)\n",
    "      start_date=start_date[0:19]\n",
    "      spark.sql(\n",
    "            f\"\"\"\n",
    "                INSERT INTO \n",
    "                    hive_metastore.generic.tbl_generic_testing_results \n",
    "                    (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                    VALUES\n",
    "                    ('{databricks_view_name}','{SAP_View_Name}','Sum of measured columns',concat('Validating Sum on ','{sap_col_name_str}', ' columns'),'{start_date}','','','','','','','FAIL','Error occured please check list of column name and count')\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d157dded-e1c4-49be-b662-0cb310ba018b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mig_exec_time(databricks_view_name, SAP_View_Name, databricks_database_name):\n",
    "  execution_time=100\n",
    "  for i in range(0,10):\n",
    "    start_time_1=datetime.now()\n",
    "    spark.sql(f\"\"\" select * from {databricks_database_name}.{databricks_view_name}\"\"\")\n",
    "    end_time_1=datetime.now()\n",
    "    end_date_1=str(end_time_1)\n",
    "    end_date_1=end_date_1[0:19]\n",
    "    start_date_1=str(start_time_1)\n",
    "    start_date_1=start_date_1[0:19]\n",
    "    execution_tym=str(end_time_1-start_time_1)\n",
    "    execution_tym=float(execution_tym[5:])\n",
    "    if execution_tym<execution_time:\n",
    "      end_date=end_date_1\n",
    "      start_date=start_date_1\n",
    "      execution_time=execution_tym\n",
    "  execution_time='0'+str(execution_time)\n",
    "\n",
    "  spark.sql(\n",
    "      f\"\"\"\n",
    "          INSERT INTO \n",
    "              hive_metastore.generic.tbl_generic_testing_results \n",
    "              (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "              VALUES\n",
    "              ('{databricks_view_name}','{SAP_View_Name}','Execution Time of databricks view','Execution time of SELECT(*) FROM databricks view','{start_date}','{end_date}','{execution_time}','','','','','NA','Test Status not required')\n",
    "      \"\"\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2e8ead-8de7-47d6-a88e-d8d90d227aec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mig_exec_time(databricks_view_name, SAP_View_Name, databricks_database_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86183a3d-6c68-45dd-96ab-206edc3f4ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Execution time of sap view\n",
    "spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Execution Time of sap view','Execution time of SELECT(*) FROM sap view','','','{execution_time_of_sap_view}','','','','','NA','Test Status not required')\n",
    "          \"\"\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edcbede5-4011-4ee5-a240-53e2213a5a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_end_datetime=datetime.now()\n",
    "notebook_end_date=str(notebook_end_datetime)[0:19]\n",
    "execution_time=str(notebook_end_datetime- notebook_start_datetime)[2:]\n",
    "execution_time=str(int(execution_time[0:2])*60+int(execution_time[3:5]))+execution_time[5:]\n",
    "spark.sql(\n",
    "          f\"\"\"\n",
    "              INSERT INTO \n",
    "                  hive_metastore.generic.tbl_generic_testing_results \n",
    "                  (Databricks_View_Name,SAP_View_Name,Test_Name,Test_Description,Test_Start_Date,Test_End_Date,Test_Execution_Time_In_Seconds,Databricks_View_Count,SAP_View_Count,Check_Difference_Count_Between_Databricks_And_SAP,Check_Difference_Sum_Between_Databricks_And_SAP,Test_validation_Status,Comment) \n",
    "                  VALUES\n",
    "                  ('{databricks_view_name}','{SAP_View_Name}','Total notebook execution time','Total time taken to execute test notebook with all the relevant tests','{notebook_start_date}','{notebook_end_date}','{execution_time}','','','','','NA','Test Status not required')\n",
    "          \"\"\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae26f20-3a46-425a-9ca2-9ccf901f296e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Test_ID</th><th>Databricks_View_Name</th><th>SAP_View_Name</th><th>Test_Name</th><th>Test_Description</th><th>Test_Start_Date</th><th>Test_End_Date</th><th>Test_Execution_Time_In_Seconds</th><th>Databricks_View_Count</th><th>SAP_View_Count</th><th>Difference_Count_Between_Databricks_And_SAP</th><th>Difference_Sum_Between_Databricks_And_SAP</th><th>Test_validation_Status</th><th>Comment</th></tr></thead><tbody><tr><td>7</td><td>v_open_ord_lead1</td><td>CA_OPEN_ORD_LEAD1</td><td>Records count on filtered column</td><td>Record count on MATNR, WERKS, LEAD1_OPEN_ORDERS filtered columns</td><td>2023-05-31 08:58:45</td><td>2023-05-31 08:58:53</td><td>07.765166</td><td>2</td><td>2</td><td>0</td><td></td><td>PASS</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "v_open_ord_lead1",
         "CA_OPEN_ORD_LEAD1",
         "Records count on filtered column",
         "Record count on MATNR, WERKS, LEAD1_OPEN_ORDERS filtered columns",
         "2023-05-31 08:58:45",
         "2023-05-31 08:58:53",
         "07.765166",
         "2",
         "2",
         "0",
         "",
         "PASS",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Test_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Databricks_View_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SAP_View_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Start_Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_End_Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Execution_Time_In_Seconds",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Databricks_View_Count",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SAP_View_Count",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Difference_Count_Between_Databricks_And_SAP",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Difference_Sum_Between_Databricks_And_SAP",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_validation_Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from hive_metastore.generic.tbl_generic_testing_results\n",
    "order by Test_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c24564-6179-4304-bc41-6cfb489dfebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1099526696387477,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Migration_Validation_Testing_Framework",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
